{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnuRWoV4vLNK"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ZkCf0ivLNM"
      },
      "outputs": [],
      "source": [
        "!pip install -r ../dev-requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00NDvxSvLNN"
      },
      "source": [
        "### Set up variables for MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h-vpYWdvLNN"
      },
      "outputs": [],
      "source": [
        "# Load .env file if it exists\n",
        "# Don't use dotenv\n",
        "!pip install python-dotenv\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('../.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A89o4LJvvLNO"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Buy4MMFvLNO",
        "outputId": "9020e1a5-bc73-4ce8-a7aa-358b684c5fb0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder = os.path.join(\"../airflow/assets\")\n",
        "dfs = []\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "        dfs.append(pd.read_csv(os.path.join(folder, file), skiprows=1, parse_dates=['date']))\n",
        "print(dfs.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeDX0579vLNO"
      },
      "source": [
        "#### Merge all data into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wloayX8WvLNO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Convert \"date\" column to datetime in all dataframes\n",
        "for df in dfs:\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S', errors=\"coerce\")\n",
        "\n",
        "# Step 2: Find the oldest and newest dates across all dataframes\n",
        "all_dates = [df['date'] for df in dfs]\n",
        "all_dates_flat = [date for sublist in all_dates for date in sublist if not pd.isnull(date)]\n",
        "\n",
        "oldest_date = min(all_dates_flat)\n",
        "newest_date = max(all_dates_flat)\n",
        "\n",
        "# Step 3: Create a new dataframe with the date range\n",
        "date_range = pd.date_range(start=oldest_date, end=newest_date, freq='H')  # Hourly frequency\n",
        "merged_df = pd.DataFrame({'date': date_range})\n",
        "\n",
        "# Step 4: Add \"close\" columns from each dataframe to the merged_df using list comprehension\n",
        "for df in dfs:\n",
        "    try:\n",
        "        ticker = df['symbol'].iloc[0]  # Assuming each dataframe has a \"ticker\" column\n",
        "        close_col_name = f'close_{ticker}'\n",
        "\n",
        "        df = df.set_index('date').sort_index()\n",
        "        df = df[~df.index.duplicated(keep='first')].reindex(date_range, method='ffill')\n",
        "\n",
        "        # Create a DataFrame with the \"date\" and \"close\" columns\n",
        "        close_data = df[df.index.isin(date_range)][['close']]\n",
        "        close_data.rename(columns={'close': close_col_name}, inplace=True)\n",
        "\n",
        "        # Merge the \"close_data\" into the \"merged_df\"\n",
        "        merged_df = pd.merge(merged_df, close_data, left_on='date', right_index=True, how='left')\n",
        "    except ValueError as e:\n",
        "        print(f'Error on coin {ticker}: {e}')\n",
        "\n",
        "\n",
        "# Now, merged_df contains the desired data with the date range and \"close_{ticker}\" columns, with missing hours filled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvdNJi5rvLNQ"
      },
      "source": [
        "### Clustering Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsru3_eMvLNQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from fastdtw import fastdtw\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "\n",
        "def preprocess_data(\n",
        "    merged_df,\n",
        "    slice_data=True,\n",
        "    scale_data=None,\n",
        "    drop_columns=None,\n",
        "    percentage_change=False\n",
        "):\n",
        "    # Optionally slice data based on the date\n",
        "    if slice_data:\n",
        "        time_series_data = merged_df[merged_df['date'] >= '2021-06-01']\n",
        "    else:\n",
        "        time_series_data = merged_df.copy()\n",
        "\n",
        "\n",
        "    # Backfill missing values\n",
        "    time_series_data = time_series_data.fillna(method='bfill')\n",
        "\n",
        "    # Optionally filter out columns\n",
        "    if drop_columns:\n",
        "        time_series_data = time_series_data.drop(drop_columns, axis=1)\n",
        "\n",
        "    # Select the time series columns for clustering and exclude 'date' column\n",
        "    df = time_series_data.copy()\n",
        "    time_series_data = time_series_data.iloc[:, 1:].values\n",
        "\n",
        "\n",
        "    # Optionally scale data (e.g., min-max scaling or log transformation)\n",
        "    if scale_data == \"min_max\":\n",
        "        time_series_data = (time_series_data - time_series_data.min()) / (time_series_data.max() - time_series_data.min())\n",
        "    elif scale_data == \"log\":\n",
        "        time_series_data = np.log(time_series_data)\n",
        "\n",
        "    # Optionally calculate percentage change\n",
        "    if percentage_change:\n",
        "        time_series_data = np.diff(time_series_data, axis=1) / time_series_data[:, :-1]\n",
        "\n",
        "    time_series_data = time_series_data.T  # Transpose the data to have time series in rows\n",
        "\n",
        "    return time_series_data, df\n",
        "\n",
        "def do_clustering(\n",
        "    merged_df,\n",
        "    cluster_range=range(2, 8),\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    slice_data=True,\n",
        "    scale_data=None,\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        "):\n",
        "    experiment_name = f\"Clustering\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    time_series_data, df = preprocess_data(merged_df, slice_data, scale_data, filter_data, percentage_change)\n",
        "\n",
        "    # Rest of your clustering code\n",
        "    distances = None\n",
        "    # Initialize an empty array to store the distances\n",
        "    distances = np.zeros((time_series_data.shape[0], time_series_data.shape[0]))\n",
        "    # Calculate DTW distances between time series columns with progress tracking\n",
        "    for i in tqdm(range(time_series_data.shape[0]), desc=\"Calculating DTW Distances\"):\n",
        "        for j in range(i, time_series_data.shape[0]):\n",
        "            distance, _ = fastdtw(time_series_data[i], time_series_data[j])\n",
        "            distances[i, j] = distance\n",
        "            distances[j, i] = distance\n",
        "\n",
        "    # Silhouette Score vs. Cluster Count\n",
        "    silhouette_scores = []\n",
        "    inertia_scores = []\n",
        "\n",
        "    # Iterate through different cluster counts\n",
        "    for n_clusters in cluster_range:\n",
        "        \n",
        "        with mlflow.start_run():\n",
        "\n",
        "            # Log parameters to MLflow\n",
        "            mlflow.log_param(\"N_clusters\", str(n_clusters))\n",
        "            mlflow.log_param(\"Clustering_Algorithm\", clustering_algorithm)\n",
        "            mlflow.log_param(\"Slice_Data\", slice_data)\n",
        "            mlflow.log_param(\"Scale_Data\", scale_data)\n",
        "            mlflow.log_param(\"Filter_Data\", filter_data)\n",
        "            mlflow.log_param(\"Percentage_Change\", percentage_change)\n",
        "\n",
        "            print(f\"Running experiment for {n_clusters} clusters...\")\n",
        "\n",
        "            if clustering_algorithm == \"KMeans\":\n",
        "                # Perform K-Means clustering with DTW distances\n",
        "                kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(distances)\n",
        "\n",
        "                # Get cluster labels\n",
        "                labels = kmeans.labels_\n",
        "                inertia_score = kmeans.inertia_\n",
        "\n",
        "                print(\"K-Means clustering completed.\")\n",
        "\n",
        "            elif clustering_algorithm == \"KMedoids\":\n",
        "                # Perform K-Medoids clustering with DTW distances\n",
        "                kmedoids = KMedoids(n_clusters=n_clusters, metric=\"precomputed\", random_state=0).fit(distances)\n",
        "\n",
        "                # Get cluster labels\n",
        "                labels = kmedoids.labels_\n",
        "                inertia_score = kmedoids.inertia_\n",
        "\n",
        "                print(\"K-Medoids clustering completed.\")\n",
        "\n",
        "            # Log clustering results and distances to MLflow\n",
        "            mlflow.log_param(\"Cluster_Count\", n_clusters)\n",
        "            mlflow.log_param(\"Distance_Matrix_Shape\", distances.shape)\n",
        "            mlflow.log_param(\"Cluster_Labels\", labels.tolist())\n",
        "\n",
        "            print(\"Clustering results logged to MLflow.\")\n",
        "\n",
        "            # Calculate Silhouette Score and Inertia\n",
        "            print(distances.shape)\n",
        "            print(distances)\n",
        "\n",
        "            silhouette = silhouette_score(distances, labels, metric=\"precomputed\")\n",
        "\n",
        "            silhouette_scores.append(silhouette)\n",
        "            inertia_scores.append(inertia_score)\n",
        "\n",
        "            mlflow.log_metric(\"Silhouette_Score\", silhouette)\n",
        "            mlflow.log_metric(\"Inertia_Score\", inertia_score)\n",
        "\n",
        "            # Define a color palette for plotting\n",
        "            palette = sns.color_palette(\"husl\", len(time_series_data))\n",
        "\n",
        "            # Visualize the clustered time series using Seaborn\n",
        "            num_rows = n_clusters\n",
        "            num_cols = 1\n",
        "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6 * num_rows))\n",
        "\n",
        "            for cluster_id in range(n_clusters):\n",
        "                ax = axes[cluster_id] if num_rows > 1 else axes\n",
        "                for series_idx, label in enumerate(labels):\n",
        "                    if label == cluster_id:\n",
        "                        series_name = df.columns[series_idx + 1]  # Get the column name (series name)\n",
        "                        sns.lineplot(data=time_series_data[series_idx], color=palette[series_idx], label=series_name, ax=ax)\n",
        "\n",
        "                ax.annotate(f\"Inertia: {inertia_score:.2f}\", xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
        "                ax.annotate(f\"Silhouette: {silhouette:.2f}\", xy=(0.05, 0.75), xycoords='axes fraction', fontsize=10)\n",
        "\n",
        "                ax.set_title(f\"Cluster {cluster_id + 1}\")\n",
        "                ax.legend(loc='upper right')  # Add legends for series in the cluster\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot to a file and log it to MLflow\n",
        "            plot_path = f\"cluster_plots_{n_clusters}_clusters.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            mlflow.log_artifact(plot_path)\n",
        "\n",
        "            print(\"Cluster visualization plot saved and logged to MLflow.\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    with mlflow.start_run(run_name=\"Scores_logging\"):\n",
        "\n",
        "        # Log parameters to MLflow\n",
        "        mlflow.log_param(\"N_clusters\", str(n_clusters))\n",
        "        mlflow.log_param(\"Clustering_Algorithm\", clustering_algorithm)\n",
        "        mlflow.log_param(\"Slice_Data\", slice_data)\n",
        "        mlflow.log_param(\"Scale_Data\", scale_data)\n",
        "        mlflow.log_param(\"Filter_Data\", filter_data)\n",
        "        mlflow.log_param(\"Percentage_Change\", percentage_change)\n",
        "\n",
        "\n",
        "        # Log the lists of Silhouette and Inertia scores\n",
        "        mlflow.log_param(\"Silhouette_Scores\", silhouette_scores)\n",
        "        mlflow.log_param(\"Inertia_Scores\", inertia_scores)\n",
        "\n",
        "        # Silhouette Score vs. Cluster Count plot\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(cluster_range, silhouette_scores, marker='o')\n",
        "        plt.title(\"Silhouette Score vs. Cluster Count\")\n",
        "        plt.xlabel(\"Cluster Count\")\n",
        "        plt.ylabel(\"Silhouette Score\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"silhouette_scores_plot.png\")\n",
        "        mlflow.log_artifact(\"silhouette_scores_plot.png\")\n",
        "\n",
        "        # Inertia Score vs. Cluster Count plot\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(cluster_range, inertia_scores, marker='o')\n",
        "        plt.title(\"Inertia Score vs. Cluster Count\")\n",
        "        plt.xlabel(\"Cluster Count\")\n",
        "        plt.ylabel(\"Inertia Score\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"inertia_scores_plot.png\")\n",
        "        mlflow.log_artifact(\"inertia_scores_plot.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYJyc8BTvLNQ"
      },
      "source": [
        "#### Clustering Analysis (on Raw Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Experiment 1: Raw Data\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    slice_data=False,\n",
        "    scale_data=None,\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    slice_data=False,\n",
        "    scale_data=None,\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "# Experiment 2: Data from 2021-06\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    slice_data=True,\n",
        "    scale_data=None,\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    slice_data=True,\n",
        "    scale_data=None,\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "# Experiment 3: Data from 2021-06 with Custom Min-Max Scaling\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    slice_data=True,\n",
        "    scale_data=\"min_max\",\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    slice_data=True,\n",
        "    scale_data=\"min_max\",\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "# Experiment 4: Data from 2021-06 with Column Filtering and Custom Min-Max Scaling\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    slice_data=True,\n",
        "    scale_data=\"min_max\",\n",
        "    filter_data=['close_XLM/USD', 'close_RSR/USD', 'close_ICP/USD', 'close_LUNA/USD'],\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    slice_data=True,\n",
        "    scale_data=\"min_max\",\n",
        "    filter_data=['close_XLM/USD', 'close_RSR/USD', 'close_ICP/USD', 'close_LUNA/USD'],\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "# Experiment 5: Custom Min-Max Scaling with all Columns\n",
        "time_series_data = merged_df.copy()\n",
        "do_clustering(\n",
        "    time_series_data,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    slice_data=False,\n",
        "    scale_data=\"min_max\",\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "do_clustering(\n",
        "    time_series_data,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    slice_data=False,\n",
        "    scale_data=\"min_max\",\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")\n",
        "\n",
        "# Experiment 6: Data from 2021-06 with Log Transformation\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    slice_data=True,\n",
        "    scale_data=\"log\",\n",
        "    filter_data=None,\n",
        "    percentage_change=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4_ZXACDRvLNP",
        "bCkX8FAvvLNR"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
