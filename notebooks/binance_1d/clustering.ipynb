{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnuRWoV4vLNK"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "x-ZkCf0ivLNM",
        "outputId": "3921f206-ca9b-4a8a-ab77-c08c82465a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: voluptuous, pygtrie, funcy, dictdiffer, antlr4-python3-runtime, zc.lockfile, vine, tzdata, tomlkit, smmap, shtab, shortuuid, semver, ruamel.yaml.clib, querystring-parser, pyspark, pyflakes, pycodestyle, pathspec, orjson, omegaconf, mypy-extensions, mccabe, Mako, gunicorn, grandalf, flufl.lock, flatten-dict, fastdtw, dvc-render, dulwich, dpath, configobj, colorama, click-didyoumean, billiard, sqltrie, ruamel.yaml, pygit2, iterative-telemetry, hydra-core, gitdb, flake8, dvc-studio-client, dvc-objects, docker, databricks-cli, click-repl, black, amqp, alembic, scikit-learn-extra, kombu, gitpython, dvc-data, asyncssh, aiohttp-retry, scmrepo, mlflow, dvc-http, celery, gto, dvc-task, dvc\n",
            "Successfully installed Mako-1.2.4 aiohttp-retry-2.8.3 alembic-1.12.0 amqp-5.1.1 antlr4-python3-runtime-4.9.3 asyncssh-2.14.0 billiard-4.1.0 black-23.10.0 celery-5.3.4 click-didyoumean-0.3.0 click-repl-0.3.0 colorama-0.4.6 configobj-5.0.8 databricks-cli-0.18.0 dictdiffer-0.9.0 docker-6.1.3 dpath-2.1.6 dulwich-0.21.6 dvc-3.27.0 dvc-data-2.18.1 dvc-http-2.30.2 dvc-objects-1.0.1 dvc-render-0.6.0 dvc-studio-client-0.15.0 dvc-task-0.3.0 fastdtw-0.3.4 flake8-6.1.0 flatten-dict-0.4.2 flufl.lock-7.1.1 funcy-2.0 gitdb-4.0.11 gitpython-3.1.40 grandalf-0.8 gto-1.4.0 gunicorn-21.2.0 hydra-core-1.3.2 iterative-telemetry-0.0.8 kombu-5.3.2 mccabe-0.7.0 mlflow-2.7.1 mypy-extensions-1.0.0 omegaconf-2.3.0 orjson-3.9.9 pathspec-0.11.2 pycodestyle-2.11.1 pyflakes-3.1.0 pygit2-1.13.1 pygtrie-2.5.0 pyspark-3.5.0 querystring-parser-1.2.4 ruamel.yaml-0.17.40 ruamel.yaml.clib-0.2.8 scikit-learn-extra-0.3.0 scmrepo-1.4.0 semver-3.0.2 shortuuid-1.0.11 shtab-1.6.4 smmap-5.0.1 sqltrie-0.8.0 tomlkit-0.12.1 tzdata-2023.3 vine-5.0.0 voluptuous-0.13.1 zc.lockfile-3.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r dev-requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00NDvxSvLNN"
      },
      "source": [
        "### Set up variables for MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_h-vpYWdvLNN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://4221-82-49-26-249.ngrok-free.app\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A89o4LJvvLNO"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Buy4MMFvLNO",
        "outputId": "960271ec-f697-4c6e-85a4-b76a0c5bb740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder = os.path.join(\"assets\")\n",
        "dfs = []\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "        dfs.append(pd.read_csv(os.path.join(folder, file), skiprows=1, parse_dates=['Date']))\n",
        "print(dfs.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeDX0579vLNO"
      },
      "source": [
        "#### Merge all data into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wloayX8WvLNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a567e4b-3f74-44f1-931a-8ee3ce5b64f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Convert \"date\" column to datetime in all dataframes\n",
        "for df in dfs:\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors=\"coerce\")\n",
        "\n",
        "# Step 2: Find the oldest and newest dates across all dataframes\n",
        "all_dates = [df['Date'] for df in dfs]\n",
        "all_dates_flat = [date for sublist in all_dates for date in sublist if not pd.isnull(date)]\n",
        "\n",
        "oldest_date = '2019-01-01'\n",
        "newest_date = max(all_dates_flat)\n",
        "\n",
        "# Step 3: Create a new dataframe with the date range\n",
        "date_range = pd.date_range(start=oldest_date, end=newest_date, freq='D')  # Daily frequency\n",
        "merged_df = pd.DataFrame({'Date': date_range})\n",
        "\n",
        "# Step 4: Add \"close\" and \"Volume USDT\" columns from each dataframe to the merged_df using list comprehension\n",
        "for df in dfs:\n",
        "    try:\n",
        "        ticker = df['Symbol'].iloc[0]  # Assuming each dataframe has a \"symbol\" column\n",
        "        close_col_name = f'close_{ticker}'\n",
        "        volume_col_name = f'Volume USDT_{ticker}'  # Replace with the actual column name if it's different in your data\n",
        "\n",
        "        df = df.set_index('Date').sort_index()\n",
        "\n",
        "        # Create DataFrames with the \"date\" and \"close\" columns\n",
        "        close_data = df[df.index.isin(date_range)][['Close']]\n",
        "        close_data.rename(columns={'Close': close_col_name}, inplace=True)\n",
        "\n",
        "        # Merge the \"close_data\" into the \"merged_df\"\n",
        "        merged_df = pd.merge(merged_df, close_data, left_on='Date', right_index=True, how='left')\n",
        "\n",
        "        # Add the \"Volume USDT\" column to the merged_df (replace 'Volume USDT' with the actual column name if it's different)\n",
        "        # merged_df[volume_col_name] = df['Volume USDT']\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f'Error on coin {ticker}: {e}')\n",
        "\n",
        "\n",
        "# print number of columns -1 of merged_df\n",
        "print(merged_df.columns.__len__()-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvdNJi5rvLNQ"
      },
      "source": [
        "### Clustering Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cFdoV6MY9XA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2aa0f83-64b1-46d4-d218-a5a18fd820cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastdtw) (1.23.5)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.18.0)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.40)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.31.0)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.2)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.8.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4.4)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.12.0)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.1.3)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.5)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.23.5)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.11.3)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.5.3)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.22)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: pyarrow<14,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.5)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: gunicorn<22 in /usr/local/lib/python3.10/dist-packages (from mlflow) (21.2.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.5.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.0.7)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow) (1.6.4)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (3.0.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.2.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.1)\n",
            "Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-extra) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastdtw\n",
        "!pip install mlflow\n",
        "!pip install scikit-learn-extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Bsru3_eMvLNQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from fastdtw import fastdtw\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "\n",
        "\n",
        "# Create a custom Min-Max scaling function\n",
        "def custom_min_max_scaling(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    scaled_column = (column - min_val) / (max_val - min_val)\n",
        "    return scaled_column\n",
        "\n",
        "\n",
        "def preprocess_data(\n",
        "    merged_df,\n",
        "    scale_data=None,\n",
        "    drop_columns=None,\n",
        "    percentage_change=False\n",
        "):\n",
        "    time_series_data = merged_df.copy()\n",
        "\n",
        "    # Optionally filter out columns\n",
        "    if drop_columns:\n",
        "        time_series_data = time_series_data.drop(drop_columns, axis=1)\n",
        "\n",
        "    # Select the time series columns for clustering and exclude 'date' column\n",
        "    time_series_data = time_series_data.iloc[:, 1:].values\n",
        "\n",
        "    # Optionally calculate percentage change\n",
        "    if percentage_change:\n",
        "        time_series_data = np.diff(time_series_data, axis=1) / time_series_data[:, :-1]\n",
        "\n",
        "    time_series_data = time_series_data.T  # Transpose the data to have time series in rows\n",
        "\n",
        "    # Optionally scale data (e.g., min-max scaling or log transformation)\n",
        "    if scale_data == \"custom_min_max\":\n",
        "        time_series_data = np.array([custom_min_max_scaling(col) for col in time_series_data])\n",
        "    elif scale_data == \"log\":\n",
        "        time_series_data = np.log(time_series_data)\n",
        "\n",
        "    return time_series_data\n",
        "\n",
        "def do_clustering(\n",
        "    merged_df,\n",
        "    cluster_range=range(2, 8),\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=None,\n",
        "    percentage_change=False\n",
        "):\n",
        "    experiment_name = f\"Clustering Binance 1D\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    time_series_data = preprocess_data(merged_df, scale_data, percentage_change)\n",
        "\n",
        "    # Rest of your clustering code\n",
        "    distances = None\n",
        "    # Initialize an empty array to store the distances\n",
        "    distances = np.zeros((time_series_data.shape[0], time_series_data.shape[0]))\n",
        "    # Calculate DTW distances between time series columns with progress tracking\n",
        "    for i in tqdm(range(time_series_data.shape[0]), desc=\"Calculating DTW Distances\"):\n",
        "        for j in range(i, time_series_data.shape[0]):\n",
        "            distance, _ = fastdtw(time_series_data[i], time_series_data[j])\n",
        "            distances[i, j] = distance\n",
        "            distances[j, i] = distance\n",
        "\n",
        "    # Silhouette Score vs. Cluster Count\n",
        "    silhouette_scores = []\n",
        "    inertia_scores = []\n",
        "\n",
        "    # Iterate through different cluster counts\n",
        "    for n_clusters in cluster_range:\n",
        "\n",
        "        with mlflow.start_run():\n",
        "\n",
        "            # Log parameters to MLflow\n",
        "            mlflow.log_params({\n",
        "              \"N_clusters\": str(n_clusters),\n",
        "              \"Clustering_Algorithm\": clustering_algorithm,\n",
        "              \"Scale_Data\": scale_data,\n",
        "              \"Percentage_Change\": percentage_change\n",
        "            })\n",
        "\n",
        "            print(f\"Running experiment for {n_clusters} clusters...\")\n",
        "\n",
        "            if clustering_algorithm == \"KMeans\":\n",
        "                # Perform K-Means clustering with DTW distances\n",
        "                kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(distances)\n",
        "\n",
        "                # Get cluster labels\n",
        "                labels = kmeans.labels_\n",
        "                inertia_score = kmeans.inertia_\n",
        "\n",
        "                print(\"K-Means clustering completed.\")\n",
        "\n",
        "            elif clustering_algorithm == \"KMedoids\":\n",
        "                # Perform K-Medoids clustering with DTW distances\n",
        "                kmedoids = KMedoids(n_clusters=n_clusters, metric=\"precomputed\", random_state=0).fit(distances)\n",
        "\n",
        "                # Get cluster labels\n",
        "                labels = kmedoids.labels_\n",
        "                inertia_score = kmedoids.inertia_\n",
        "\n",
        "                print(\"K-Medoids clustering completed.\")\n",
        "\n",
        "            mlflow.log_params({\n",
        "                \"Distance_Matrix_Shape\": distances.shape,\n",
        "                \"Cluster_Labels\": [pair for pair in zip(merged_df.columns[1:], labels.tolist())]\n",
        "            })\n",
        "\n",
        "            print(\"Clustering results logged to MLflow.\")\n",
        "\n",
        "            silhouette = silhouette_score(distances, labels, metric=\"precomputed\")\n",
        "\n",
        "            silhouette_scores.append(silhouette)\n",
        "            inertia_scores.append(inertia_score)\n",
        "\n",
        "            mlflow.log_metrics({\n",
        "                \"Silhouette_Score\": silhouette,\n",
        "                \"Inertia_Score\": inertia_score\n",
        "            })\n",
        "\n",
        "            # Define a color palette for plotting\n",
        "            palette = sns.color_palette(\"husl\", len(time_series_data))\n",
        "\n",
        "            # Visualize the clustered time series using Seaborn\n",
        "            num_rows = n_clusters\n",
        "            num_cols = 1\n",
        "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6 * num_rows))\n",
        "\n",
        "            for cluster_id in range(n_clusters):\n",
        "                ax = axes[cluster_id] if num_rows > 1 else axes\n",
        "                for series_idx, label in enumerate(labels):\n",
        "                    if label == cluster_id:\n",
        "                        series_name = merged_df.columns[series_idx + 1]  # Get the column name (series name)\n",
        "                        sns.lineplot(data=time_series_data[series_idx], color=palette[series_idx], label=series_name, ax=ax)\n",
        "\n",
        "                ax.annotate(f\"Inertia: {inertia_score:.2f}\", xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
        "                ax.annotate(f\"Silhouette: {silhouette:.2f}\", xy=(0.05, 0.75), xycoords='axes fraction', fontsize=10)\n",
        "\n",
        "                ax.set_title(f\"Cluster {cluster_id + 1}\")\n",
        "                ax.legend(loc='upper right')  # Add legends for series in the cluster\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot to a file and log it to MLflow\n",
        "            plot_path = f\"cluster_plots_{n_clusters}_clusters.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            mlflow.log_artifact(plot_path)\n",
        "\n",
        "            print(\"Cluster visualization plot saved and logged to MLflow.\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    with mlflow.start_run(run_name=\"Scores_logging\"):\n",
        "\n",
        "        # Log parameters to MLflow\n",
        "        mlflow.log_param(\"N_clusters\", str(n_clusters))\n",
        "        mlflow.log_param(\"Clustering_Algorithm\", clustering_algorithm)\n",
        "        mlflow.log_param(\"Scale_Data\", scale_data)\n",
        "        mlflow.log_param(\"Percentage_Change\", percentage_change)\n",
        "\n",
        "\n",
        "        # Log the lists of Silhouette and Inertia scores\n",
        "        mlflow.log_param(\"Silhouette_Scores\", silhouette_scores)\n",
        "        mlflow.log_param(\"Inertia_Scores\", inertia_scores)\n",
        "\n",
        "        # Silhouette Score vs. Cluster Count plot\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(cluster_range, silhouette_scores, marker='o')\n",
        "        plt.title(\"Silhouette Score vs. Cluster Count\")\n",
        "        plt.xlabel(\"Cluster Count\")\n",
        "        plt.ylabel(\"Silhouette Score\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"silhouette_scores_plot.png\")\n",
        "        mlflow.log_artifact(\"silhouette_scores_plot.png\")\n",
        "\n",
        "        # Inertia Score vs. Cluster Count plot\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(cluster_range, inertia_scores, marker='o')\n",
        "        plt.title(\"Inertia Score vs. Cluster Count\")\n",
        "        plt.xlabel(\"Cluster Count\")\n",
        "        plt.ylabel(\"Inertia Score\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"inertia_scores_plot.png\")\n",
        "        mlflow.log_artifact(\"inertia_scores_plot.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYJyc8BTvLNQ"
      },
      "source": [
        "#### Clustering Analysis (on Raw Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9o5BVJC8wxk",
        "outputId": "f73ab56e-c99e-499e-bda2-961c8e31d92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023/10/21 15:19:51 INFO mlflow.tracking.fluent: Experiment with name 'Clustering Binance 1D' does not exist. Creating a new experiment.\n",
            "Calculating DTW Distances: 100%|██████████| 17/17 [00:00<00:00, 96.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for 2 clusters...\n",
            "K-Means clustering completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering results logged to MLflow.\n",
            "Cluster visualization plot saved and logged to MLflow.\n"
          ]
        }
      ],
      "source": [
        "# Experiment 1: Raw Data\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=None,\n",
        ")\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    scale_data=None,\n",
        ")\n",
        "\n",
        "\n",
        "# Experiment 2: Min-Max Scaling\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=\"custom_min_max\",\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    scale_data=\"custom_min_max\",\n",
        ")\n",
        "\n",
        "\n",
        "# Experiment 3: Log scaling\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=\"log\",\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    scale_data=\"log\",\n",
        ")\n",
        "\n",
        "\n",
        "# Experiment 4: Percentage change\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    percentage_change=True\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    percentage_change=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}