{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnuRWoV4vLNK"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "x-ZkCf0ivLNM",
        "outputId": "3921f206-ca9b-4a8a-ab77-c08c82465a34"
      },
      "outputs": [],
      "source": [
        "!pip install -r ../../dev-requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00NDvxSvLNN"
      },
      "source": [
        "### Set up variables for MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h-vpYWdvLNN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://df79-82-49-26-249.ngrok-free.app\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A89o4LJvvLNO"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Buy4MMFvLNO",
        "outputId": "960271ec-f697-4c6e-85a4-b76a0c5bb740"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder = os.path.join(\"../../airflow/assets/binance_1d\")\n",
        "dfs = []\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "        dfs.append(pd.read_csv(os.path.join(folder, file), skiprows=1, parse_dates=['Date']))\n",
        "print(dfs.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeDX0579vLNO"
      },
      "source": [
        "#### Merge all data into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wloayX8WvLNO",
        "outputId": "8a567e4b-3f74-44f1-931a-8ee3ce5b64f3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Convert \"date\" column to datetime in all dataframes\n",
        "for df in dfs:\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors=\"coerce\")\n",
        "\n",
        "# Step 2: Find the oldest and newest dates across all dataframes\n",
        "all_dates = [df['Date'] for df in dfs]\n",
        "all_dates_flat = [date for sublist in all_dates for date in sublist if not pd.isnull(date)]\n",
        "\n",
        "oldest_date = '2019-01-01'\n",
        "newest_date = max(all_dates_flat)\n",
        "\n",
        "# Step 3: Create a new dataframe with the date range\n",
        "date_range = pd.date_range(start=oldest_date, end=newest_date, freq='D')  # Daily frequency\n",
        "merged_df = pd.DataFrame({'Date': date_range})\n",
        "\n",
        "# Step 4: Add \"close\" and \"Volume USDT\" columns from each dataframe to the merged_df using list comprehension\n",
        "for df in dfs:\n",
        "    try:\n",
        "        ticker = df['Symbol'].iloc[0]  # Assuming each dataframe has a \"symbol\" column\n",
        "        close_col_name = f'close_{ticker}'\n",
        "        volume_col_name = f'Volume USDT_{ticker}'  # Replace with the actual column name if it's different in your data\n",
        "\n",
        "        df = df.set_index('Date').sort_index()\n",
        "\n",
        "        # Create DataFrames with the \"date\" and \"close\" columns\n",
        "        close_data = df[df.index.isin(date_range)][['Close']]\n",
        "        close_data.rename(columns={'Close': close_col_name}, inplace=True)\n",
        "\n",
        "        # Merge the \"close_data\" into the \"merged_df\"\n",
        "        merged_df = pd.merge(merged_df, close_data, left_on='Date', right_index=True, how='left')\n",
        "\n",
        "        # Add the \"Volume USDT\" column to the merged_df (replace 'Volume USDT' with the actual column name if it's different)\n",
        "        # merged_df[volume_col_name] = df['Volume USDT']\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f'Error on coin {ticker}: {e}')\n",
        "\n",
        "\n",
        "# print number of columns -1 of merged_df\n",
        "print(merged_df.columns.__len__()-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvdNJi5rvLNQ"
      },
      "source": [
        "### Clustering Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsru3_eMvLNQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from fastdtw import fastdtw\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "\n",
        "\n",
        "# Create a custom Min-Max scaling function\n",
        "def custom_min_max_scaling(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    scaled_column = (column - min_val) / (max_val - min_val)\n",
        "    return scaled_column\n",
        "\n",
        "\n",
        "def preprocess_data(\n",
        "    merged_df,\n",
        "    scale_data=None,\n",
        "    percentage_change=False\n",
        "):\n",
        "    time_series_data = merged_df.copy()\n",
        "\n",
        "    # Select the time series columns for clustering and exclude 'date' column\n",
        "    time_series_data = time_series_data.iloc[:, 1:].values\n",
        "\n",
        "    # Optionally calculate percentage change\n",
        "    if percentage_change:\n",
        "        time_series_data = np.diff(time_series_data, axis=0) / time_series_data[:-1, :]\n",
        "\n",
        "    time_series_data = time_series_data.T  # Transpose the data to have time series in rows\n",
        "\n",
        "    # Optionally scale data (e.g., min-max scaling or log transformation)\n",
        "    if scale_data == \"custom_min_max\":\n",
        "        time_series_data = np.array([custom_min_max_scaling(col) for col in time_series_data])\n",
        "    elif scale_data == \"log\":\n",
        "        time_series_data = np.log(time_series_data)\n",
        "\n",
        "    return time_series_data\n",
        "\n",
        "def do_clustering(\n",
        "    merged_df,\n",
        "    cluster_range=range(2, 8),\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=None,\n",
        "    percentage_change=False\n",
        "):\n",
        "    experiment_name = f\"Clustering Binance 1D\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    time_series_data = preprocess_data(merged_df, scale_data, percentage_change)\n",
        "\n",
        "    # Rest of your clustering code\n",
        "    distances = None\n",
        "    # Initialize an empty array to store the distances\n",
        "    distances = np.zeros((time_series_data.shape[0], time_series_data.shape[0]))\n",
        "    # Calculate DTW distances between time series columns with progress tracking\n",
        "    for i in tqdm(range(time_series_data.shape[0]), desc=\"Calculating DTW Distances\"):\n",
        "        for j in range(i, time_series_data.shape[0]):\n",
        "            distance, _ = fastdtw(time_series_data[i], time_series_data[j])\n",
        "            distances[i, j] = distance\n",
        "            distances[j, i] = distance\n",
        "\n",
        "    # Silhouette Score vs. Cluster Count\n",
        "    silhouette_scores = []\n",
        "    inertia_scores = []\n",
        "\n",
        "    # Iterate through different cluster counts\n",
        "    for n_clusters in cluster_range:\n",
        "\n",
        "        with mlflow.start_run():\n",
        "\n",
        "            # Log parameters to MLflow\n",
        "            mlflow.log_params({\n",
        "              \"N_clusters\": str(n_clusters),\n",
        "              \"Clustering_Algorithm\": clustering_algorithm,\n",
        "              \"Scale_Data\": scale_data,\n",
        "              \"Percentage_Change\": percentage_change\n",
        "            })\n",
        "\n",
        "            print(f\"Running experiment for {n_clusters} clusters...\")\n",
        "\n",
        "            if clustering_algorithm == \"KMeans\":\n",
        "                # Perform K-Means clustering with DTW distances\n",
        "                kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(distances)\n",
        "\n",
        "                # Get cluster labels\n",
        "                labels = kmeans.labels_\n",
        "                inertia_score = kmeans.inertia_\n",
        "\n",
        "                print(\"K-Means clustering completed.\")\n",
        "\n",
        "            elif clustering_algorithm == \"KMedoids\":\n",
        "                # Perform K-Medoids clustering with DTW distances\n",
        "                kmedoids = KMedoids(n_clusters=n_clusters, metric=\"precomputed\", random_state=0).fit(distances)\n",
        "\n",
        "                # Get cluster labels\n",
        "                labels = kmedoids.labels_\n",
        "                inertia_score = kmedoids.inertia_\n",
        "\n",
        "                print(\"K-Medoids clustering completed.\")\n",
        "\n",
        "            mlflow.log_params({\n",
        "                \"Distance_Matrix_Shape\": distances.shape,\n",
        "                \"Cluster_Labels\": [pair for pair in zip(merged_df.columns[1:], labels.tolist())]\n",
        "            })\n",
        "\n",
        "            print(\"Clustering results logged to MLflow.\")\n",
        "\n",
        "            silhouette = silhouette_score(distances, labels, metric=\"precomputed\")\n",
        "\n",
        "            silhouette_scores.append(silhouette)\n",
        "            inertia_scores.append(inertia_score)\n",
        "\n",
        "            mlflow.log_metrics({\n",
        "                \"Silhouette_Score\": silhouette,\n",
        "                \"Inertia_Score\": inertia_score\n",
        "            })\n",
        "\n",
        "            # Define a color palette for plotting\n",
        "            palette = sns.color_palette(\"husl\", len(time_series_data))\n",
        "\n",
        "            # Visualize the clustered time series using Seaborn\n",
        "            num_rows = n_clusters\n",
        "            num_cols = 1\n",
        "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6 * num_rows))\n",
        "\n",
        "            for cluster_id in range(n_clusters):\n",
        "                ax = axes[cluster_id] if num_rows > 1 else axes\n",
        "                for series_idx, label in enumerate(labels):\n",
        "                    if label == cluster_id:\n",
        "                        series_name = merged_df.columns[series_idx + 1]  # Get the column name (series name)\n",
        "                        sns.lineplot(data=time_series_data[series_idx], color=palette[series_idx], label=series_name, ax=ax)\n",
        "\n",
        "                ax.annotate(f\"Inertia: {inertia_score:.2f}\", xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
        "                ax.annotate(f\"Silhouette: {silhouette:.2f}\", xy=(0.05, 0.75), xycoords='axes fraction', fontsize=10)\n",
        "\n",
        "                ax.set_title(f\"Cluster {cluster_id + 1}\")\n",
        "                ax.legend(loc='upper right')  # Add legends for series in the cluster\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot to a file and log it to MLflow\n",
        "            plot_path = f\"cluster_plots_{n_clusters}_clusters.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            mlflow.log_artifact(plot_path)\n",
        "\n",
        "            print(\"Cluster visualization plot saved and logged to MLflow.\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    with mlflow.start_run(run_name=\"Scores_logging\"):\n",
        "\n",
        "        # Log parameters to MLflow\n",
        "        mlflow.log_param(\"N_clusters\", str(n_clusters))\n",
        "        mlflow.log_param(\"Clustering_Algorithm\", clustering_algorithm)\n",
        "        mlflow.log_param(\"Scale_Data\", scale_data)\n",
        "        mlflow.log_param(\"Percentage_Change\", percentage_change)\n",
        "\n",
        "\n",
        "        # Log the lists of Silhouette and Inertia scores\n",
        "        mlflow.log_param(\"Silhouette_Scores\", silhouette_scores)\n",
        "        mlflow.log_param(\"Inertia_Scores\", inertia_scores)\n",
        "\n",
        "        # Silhouette Score vs. Cluster Count plot\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(cluster_range, silhouette_scores, marker='o')\n",
        "        plt.title(\"Silhouette Score vs. Cluster Count\")\n",
        "        plt.xlabel(\"Cluster Count\")\n",
        "        plt.ylabel(\"Silhouette Score\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"silhouette_scores_plot.png\")\n",
        "        mlflow.log_artifact(\"silhouette_scores_plot.png\")\n",
        "\n",
        "        # Inertia Score vs. Cluster Count plot\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(cluster_range, inertia_scores, marker='o')\n",
        "        plt.title(\"Inertia Score vs. Cluster Count\")\n",
        "        plt.xlabel(\"Cluster Count\")\n",
        "        plt.ylabel(\"Inertia Score\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"inertia_scores_plot.png\")\n",
        "        mlflow.log_artifact(\"inertia_scores_plot.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYJyc8BTvLNQ"
      },
      "source": [
        "#### Clustering Analysis (on Raw Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9o5BVJC8wxk",
        "outputId": "f73ab56e-c99e-499e-bda2-961c8e31d92b"
      },
      "outputs": [],
      "source": [
        "# Experiment 1: Raw Data\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=None,\n",
        ")\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    scale_data=None,\n",
        ")\n",
        "\n",
        "\n",
        "# Experiment 2: Min-Max Scaling\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=\"custom_min_max\",\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    scale_data=\"custom_min_max\",\n",
        ")\n",
        "\n",
        "\n",
        "# Experiment 3: Log scaling\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    scale_data=\"log\",\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    scale_data=\"log\",\n",
        ")\n",
        "\n",
        "\n",
        "# Experiment 4: Percentage change\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMeans\",\n",
        "    percentage_change=True\n",
        ")\n",
        "\n",
        "do_clustering(\n",
        "    merged_df,\n",
        "    clustering_algorithm=\"KMedoids\",\n",
        "    percentage_change=True\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
