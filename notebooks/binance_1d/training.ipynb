{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET UP ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../dev-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file if it exists\n",
    "# Don't use dotenv\n",
    "!pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "folder = os.path.join(\"../../airflow/assets/binance_1d\")\n",
    "dfs = []\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        dfs.append(pd.read_csv(os.path.join(folder, file), skiprows=1, parse_dates=['Date']))\n",
    "print(dfs.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert \"date\" column to datetime in all dataframes\n",
    "for df in dfs:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors=\"coerce\")\n",
    "\n",
    "# Step 2: Find the oldest and newest dates across all dataframes\n",
    "all_dates = [df['Date'] for df in dfs]\n",
    "all_dates_flat = [date for sublist in all_dates for date in sublist if not pd.isnull(date)]\n",
    "\n",
    "oldest_date = '2019-01-01'\n",
    "newest_date = max(all_dates_flat)\n",
    "\n",
    "# Step 3: Create a new dataframe with the date range\n",
    "date_range = pd.date_range(start=oldest_date, end=newest_date, freq='D')  # Daily frequency\n",
    "merged_df = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "# Step 4: Add \"close\" and \"Volume USDT\" columns from each dataframe to the merged_df using list comprehension\n",
    "for df in dfs:\n",
    "    try:\n",
    "        ticker = df['Symbol'].iloc[0]  # Assuming each dataframe has a \"symbol\" column\n",
    "        close_col_name = f'close_{ticker}'\n",
    "        volume_col_name = f'Volume USDT_{ticker}'  # Replace with the actual column name if it's different in your data\n",
    "\n",
    "        df = df.set_index('Date').sort_index()\n",
    "\n",
    "        # Create DataFrames with the \"date\" and \"close\" columns\n",
    "        close_data = df[df.index.isin(date_range)][['Close']]\n",
    "        close_data.rename(columns={'Close': close_col_name}, inplace=True)\n",
    "\n",
    "        # Merge the \"close_data\" into the \"merged_df\"\n",
    "        merged_df = pd.merge(merged_df, close_data, left_on='Date', right_index=True, how='left')\n",
    "\n",
    "        # Add the \"Volume USDT\" column to the merged_df (replace 'Volume USDT' with the actual column name if it's different)\n",
    "        # merged_df[volume_col_name] = df['Volume USDT']\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f'Error on coin {ticker}: {e}')\n",
    "\n",
    "\n",
    "# print number of columns -1 of merged_df\n",
    "print(merged_df.columns.__len__()-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD Experiment from MLFLOW\n",
    "\n",
    "Run \n",
    "\n",
    "``` python\n",
    "cd mlflow\n",
    "mlflow server\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "experiment_id = \"110357928989408424\"\n",
    "run_id = \"35f1bb80732f433297fda78e6638feab\"\n",
    "\n",
    "# Mlfow Section\n",
    "experiments = mlflow.search_runs(experiment_ids=experiment_id)\n",
    "experiment = experiments.loc[experiments['run_id'] == run_id]\n",
    "\n",
    "# Use eval() to convert the string to a list of tuples\n",
    "data_list = eval(experiment[\"params.Cluster_Labels\"].tolist()[0])\n",
    "\n",
    "# Convert the list of tuples to a dictionary\n",
    "data_dict = dict(data_list)\n",
    "\n",
    "# Create a map where keys are the values from the original dictionary\n",
    "cripto_clusters = {}\n",
    "for key, value in data_dict.items():\n",
    "    if value in cripto_clusters:\n",
    "        cripto_clusters[value].append(key)\n",
    "    else:\n",
    "        cripto_clusters[value] = [key]\n",
    "\n",
    "clusters_data = {}\n",
    "\n",
    "# loop on key and value of cripto_clusters\n",
    "for cluster, criptos in cripto_clusters.items():\n",
    "    _criptos = criptos + ['Date']\n",
    "    clusters_data[cluster] = merged_df[_criptos]\n",
    "\n",
    "# Clusters now contains a dictionary with the cluster number as key and the dataframe with the criptos as value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adfuller_test(time_series, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller (ADF) test for stationarity.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: A pandas Series or NumPy array containing the time series data.\n",
    "    - significance_level: The significance level for the test (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - ADF test result and p-value.\n",
    "    - A string indicating the stationarity based on the p-value.\n",
    "    \"\"\"\n",
    "\n",
    "    result = adfuller(time_series)\n",
    "    adf_statistic = result[0]\n",
    "    p_value = result[1]\n",
    "\n",
    "    if p_value <= significance_level:\n",
    "        stationarity = \"Stationary (p <= {0})\".format(significance_level)\n",
    "    else:\n",
    "        stationarity = \"Non-Stationary (p > {0})\".format(significance_level)\n",
    "\n",
    "    return adf_statistic, p_value, stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_and_plot_price_changes(cluster_data, column_name, window_size=10):\n",
    "    \"\"\"\n",
    "    Calculate and plot first-order percentage differences, rolling mean, and rolling standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A pandas DataFrame containing a 'date' index and the specified cryptocurrency price column.\n",
    "    - column_name: The name of the column representing cryptocurrency prices.\n",
    "    - window_size: The window size for the rolling statistics (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - None (plots the results).\n",
    "    \"\"\"\n",
    "    data = cluster_data.copy()\n",
    "    if 'Date' not in data.columns:\n",
    "        raise ValueError(\"The 'data' DataFrame must contain a 'date' column as the index.\")\n",
    "\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"The specified column '{column_name}' is not found in the DataFrame.\")\n",
    "    \n",
    "    # Calculate daily percentage changes\n",
    "    data['PriceChange'] = data[column_name].pct_change() * 100\n",
    "\n",
    "    # Calculate rolling mean and rolling standard deviation\n",
    "    data['RollingMean'] = data['PriceChange'].rolling(window=window_size).mean()\n",
    "    data['RollingStd'] = data['PriceChange'].rolling(window=window_size).std()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(data.index, data['PriceChange'], label='Price Change (%)', color='blue')\n",
    "    plt.plot(data.index, data['RollingMean'], label=f'Rolling Mean ({window_size}-hour)', color='green')\n",
    "    plt.plot(data.index, data['RollingStd'], label=f'Rolling Std Deviation ({window_size}-hour)', color='red')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Percentage Change / Rolling Statistics')\n",
    "    plt.title(f'{column_name} Price Changes and Rolling Statistics')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}\\n')\n",
    "    for cripto in clusters_data[cluster]:\n",
    "        if cripto != 'Date':\n",
    "            print(f'Analyzing: {cripto}')\n",
    "\n",
    "            adf_statistic, p_value, stationarity = adfuller_test(clusters_data[cluster][cripto])\n",
    "            print(\"ADF Statistic:\", adf_statistic)\n",
    "            print(\"p-value:\", p_value)\n",
    "            print(\"Stationarity:\", stationarity)\n",
    "\n",
    "            calculate_and_plot_price_changes(clusters_data[cluster], cripto)\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Time Series - Vector Auto Regression (VAR)\n",
    "\n",
    "[Source](https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "import mlflow\n",
    "\n",
    "def mts_var_training(cluster_data, \n",
    "                     apply_differencing: bool = False,\n",
    "                     max_lag: int = 10):\n",
    "    \n",
    "    experiment_name = f\"Training Binance 1D - 3 Cluster\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    train, valid, test = data_preparation(cluster_data, apply_differencing)\n",
    "    cols = cluster_data.columns.drop('Date')\n",
    "\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "\n",
    "        # Log parameters to MLflow\n",
    "        mlflow.log_params({\n",
    "            \"Training_Model\": \"VAR\",\n",
    "            \"Diff\": apply_differencing,\n",
    "            \"Max_Lag\": max_lag,\n",
    "            \"Criptos\": cols\n",
    "        })\n",
    "\n",
    "        model_fit, order, trend, aic = find_best_model(train, max_lag)\n",
    "\n",
    "        # make prediction on validation\n",
    "        prediction = model_fit.forecast(model_fit.endog, steps=len(valid))\n",
    "\n",
    "        #converting predictions to dataframe\n",
    "        pred = pd.DataFrame(index=range(0, len(prediction)), columns=[cols]) \n",
    "        for j in range(0, len(cols)):\n",
    "            for i in range(0, len(prediction)):\n",
    "                pred.iloc[i, j] = prediction[i][j]\n",
    "\n",
    "        #check rmse\n",
    "        for cripto in cols:\n",
    "            preds = pred[cripto]\n",
    "            valids = valid[cripto]\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            df['pred'] = preds\n",
    "            df['valid'] = valids.values\n",
    "            df.index = valids.index\n",
    "\n",
    "            rmse = sqrt(mean_squared_error(preds, valids))\n",
    "            print('RMSE value for', cripto, 'is : ', rmse)\n",
    "            mlflow.log_param(f\"RMSE_{cripto.split('_')[1]}\", rmse)\n",
    "\n",
    "            # Create a plot and specify colors for each time series                     \n",
    "            plot_path = f\"{cripto}_prediction_plot.png\"\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(df['pred'], label='Predictions', color='blue')\n",
    "            plt.plot(df['valid'], label='Actual', color='red')\n",
    "\n",
    "            # Add labels, title, and legend\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title('Two Time Series')\n",
    "            plt.legend(loc='upper right')\n",
    "\n",
    "            plt.annotate(f\"RMSE: {rmse}\", xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
    "\n",
    "            plt.savefig(plot_path)\n",
    "            plt.show()\n",
    "            mlflow.log_artifact(plot_path)\n",
    "            os.remove(plot_path)\n",
    "\n",
    "        mlflow.log_params({\n",
    "            \"Order\": order,\n",
    "            \"Trend\": trend,\n",
    "            \"AIC\": aic\n",
    "        })\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    return pred, valid\n",
    "\n",
    "\n",
    "def find_best_model(train_data,\n",
    "                    max_lag,\n",
    "                    trends=['c', 'ct', 'ctt', 'n']):\n",
    "    best_order = None\n",
    "    best_aic = float('inf')  # Initialize with positive infinity\n",
    "    best_model = None\n",
    "    best_trend = None\n",
    "\n",
    "    for p in range(1, max_lag + 1):\n",
    "        for trend in trends:\n",
    "            # Fit a VAR model with the specified lag order and trend\n",
    "            model = VAR(train_data)\n",
    "            model_fit = model.fit(p, trend)\n",
    "            \n",
    "            # Calculate AIC for the current model\n",
    "            current_aic = model_fit.aic\n",
    "            \n",
    "            # Check if the current AIC is the lowest so far\n",
    "            if current_aic < best_aic:\n",
    "                best_aic = current_aic\n",
    "                best_order = p\n",
    "                best_model = model_fit\n",
    "                best_trend = trend\n",
    "    \n",
    "    print('Best lag order:', best_order)\n",
    "    print('Best trend:', best_trend)\n",
    "    print('Best AIC:', best_aic)\n",
    "\n",
    "    return best_model, best_order, best_trend, best_aic\n",
    "\n",
    "\n",
    "def data_preparation(cluster_data,\n",
    "                     apply_differencing=False):\n",
    "    # Data preparation\n",
    "    \n",
    "    # Step 1: Remove the \"date\" column\n",
    "    data = cluster_data.drop(['Date'], axis=1)\n",
    "\n",
    "    if apply_differencing:\n",
    "        data = data.diff().dropna()\n",
    "        # Step 2: Set the \"date\" column as the index\n",
    "        data.index = pd.to_datetime(cluster_data.Date.iloc[1:], format='%Y-%m-%d')  # Modify the format as needed\n",
    "    else:\n",
    "        # Step 2: Set the \"date\" column as the index\n",
    "        data.index = pd.to_datetime(cluster_data.Date, format='%Y-%m-%d')  # Modify the format as needed\n",
    "\n",
    "    data = data.asfreq('D')  # Specify the frequency as 'D' for daily data\n",
    "\n",
    "    # Step 3: Dealing with missing values --> already done\n",
    "\n",
    "    # Step 4: Split the data into train and test sets\n",
    "    train_size = int(len(data) * 0.7)\n",
    "    test_size = (len(data) - train_size) / 2\n",
    "\n",
    "    train = data.iloc[0:train_size] \n",
    "    valid = data.iloc[train_size:int(len(data) - test_size)]\n",
    "    test = data.iloc[int(train_size + test_size):len(data)]\n",
    "\n",
    "    # print('train size: ', len(train))\n",
    "    # print('valid size: ', len(valid))\n",
    "    # print('test size: ', len(test))\n",
    "    # print('total size: ', len(data))\n",
    "    # print(\"\\n\")\n",
    "    assert len(data) == len(train) + len(valid) + len(test)\n",
    "\n",
    "    return train, valid, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}\\n')\n",
    "\n",
    "    pred, valid = mts_var_training(clusters_data[cluster],\n",
    "                     apply_differencing=False, \n",
    "                     max_lag=20)\n",
    "\n",
    "    print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}\\n')\n",
    "\n",
    "    pred, valid = mts_var_training(clusters_data[cluster],\n",
    "                    apply_differencing=True,\n",
    "                    max_lag=10)\n",
    "\n",
    "    print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Time Series Forecasting with Deep Learning\n",
    "\n",
    "[Source 1](https://towardsdatascience.com/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf) \\\n",
    "[Source 2](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def get_train_test_data(cluster_data, n_steps=10, test_size=0.2, shuffle=False):\n",
    "\n",
    "    data = cluster_data.copy() \n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Date'])  # Convert the 'date' column to datetime\n",
    "\n",
    "    # Sort the data by date\n",
    "    data = data.sort_values(by='Date')\n",
    "\n",
    "    # Define the number of previous time steps to consider for prediction\n",
    "    n_steps = 10  # You can adjust this value\n",
    "\n",
    "    # Create input data by shifting prices to create sequences\n",
    "    X = data.drop(columns=['Date']).values\n",
    "    X_seq = [X[i:i + n_steps] for i in range(len(X) - n_steps)]\n",
    "\n",
    "    # Shift the closing price to predict the next closing price\n",
    "    y = data.drop(columns=['Date']).shift(-n_steps).values\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    _X_train, X_test, _y_train, y_test = train_test_split(X_seq, y[:-n_steps], test_size=0.2, shuffle=False)\n",
    "\n",
    "    # split the training set into training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(_X_train, _y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Reshape the data to 3D for LSTM\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    X_valid = np.array(X_valid)\n",
    "    y_valid = np.array(y_valid)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('X_valid shape:', X_valid.shape)\n",
    "    print('y_valid shape:', y_valid.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Create a function to calculate RMSE\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def lstm_training(cluster_data, n_steps=10, test_size=0.2, shuffle=False):\n",
    "    data = cluster_data.copy()\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = get_train_test_data(data, n_steps, test_size, shuffle)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, activation='relu', input_shape=(n_steps, X_train.shape[2]), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(X_train.shape[2])\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_valid, y_valid))\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and print RMSE\n",
    "    for i, cripto in enumerate(data.columns[:-1]):\n",
    "        rmse = calculate_rmse(y_test[:, i], y_pred[:, i])\n",
    "        print(f'Root Mean Squared Error (RMSE) for {cripto}: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}\\n')\n",
    "    lstm_training(clusters_data[cluster])\n",
    "    print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Foo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Define a function to calculate RMSE\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Simulate the Foo Model\n",
    "def foo_model(x):\n",
    "    #x = x[1]\n",
    "    return x + (random.randint(-1, 1) * 0.2 * x)\n",
    "\n",
    "def foo_model_training(cluster_data):\n",
    "    _, X_valid, _, _, y_valid, _ = get_train_test_data(cluster_data)\n",
    "    data = cluster_data.copy()\n",
    "    for i, cripto in enumerate(data.columns[:-1]):\n",
    "        predictions = foo_model(clusters_data[cluster][\"close_EOSUSDT\"].values)\n",
    "        rmse = calculate_rmse(y_valid[i], predictions)\n",
    "        print(f'Root Mean Squared Error (RMSE) for {cripto}: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_data[cluster][\"close_EOSUSDT\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}\\n')\n",
    "    foo_model_training(clusters_data[cluster])\n",
    "    print(\"\\n---------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price-oracle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
