{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnuRWoV4vLNK"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ZkCf0ivLNM"
      },
      "outputs": [],
      "source": [
        "!pip install -r ../dev-requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00NDvxSvLNN"
      },
      "source": [
        "### Set up variables for MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h-vpYWdvLNN",
        "outputId": "284def8d-9659-4cae-9b81-e5a3077a37e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load .env file if it exists\n",
        "# Don't use dotenv\n",
        "!pip install python-dotenv\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('../.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A89o4LJvvLNO"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Buy4MMFvLNO",
        "outputId": "7eb24cf5-e102-48dd-d14f-4f9e617a9756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder = os.path.join(\"/content/assets\")\n",
        "dfs = []\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "        dfs.append(pd.read_csv(os.path.join(folder, file), skiprows=1, parse_dates=['date']))\n",
        "print(dfs.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeDX0579vLNO"
      },
      "source": [
        "#### Merge all data into one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wloayX8WvLNO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Convert \"date\" column to datetime in all dataframes\n",
        "for df in dfs:\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S', errors=\"coerce\")\n",
        "\n",
        "# Step 2: Find the oldest and newest dates across all dataframes\n",
        "all_dates = [df['date'] for df in dfs]\n",
        "all_dates_flat = [date for sublist in all_dates for date in sublist if not pd.isnull(date)]\n",
        "\n",
        "oldest_date = min(all_dates_flat)\n",
        "newest_date = max(all_dates_flat)\n",
        "\n",
        "# Step 3: Create a new dataframe with the date range\n",
        "date_range = pd.date_range(start=oldest_date, end=newest_date, freq='H')  # Hourly frequency\n",
        "merged_df = pd.DataFrame({'date': date_range})\n",
        "\n",
        "# Step 4: Add \"close\" columns from each dataframe to the merged_df using list comprehension\n",
        "for df in dfs:\n",
        "    try:\n",
        "        ticker = df['symbol'].iloc[0]  # Assuming each dataframe has a \"ticker\" column\n",
        "        close_col_name = f'close_{ticker}'\n",
        "\n",
        "        df = df.set_index('date').sort_index()\n",
        "        df = df[~df.index.duplicated(keep='first')].reindex(date_range, method='ffill')\n",
        "\n",
        "        # Create a DataFrame with the \"date\" and \"close\" columns\n",
        "        close_data = df[df.index.isin(date_range)][['close']]\n",
        "        close_data.rename(columns={'close': close_col_name}, inplace=True)\n",
        "\n",
        "        # Merge the \"close_data\" into the \"merged_df\"\n",
        "        merged_df = pd.merge(merged_df, close_data, left_on='date', right_index=True, how='left')\n",
        "    except ValueError as e:\n",
        "        print(f'Error on coin {ticker}: {e}')\n",
        "\n",
        "\n",
        "# Now, merged_df contains the desired data with the date range and \"close_{ticker}\" columns, with missing hours filled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvdNJi5rvLNQ"
      },
      "source": [
        "### Clustering Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bsru3_eMvLNQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from fastdtw import fastdtw\n",
        "from tqdm import tqdm\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def do_clustering(\n",
        "    time_series_data,\n",
        "    distances=None,\n",
        "    experiment_name=\"g_KMeans_Clustering\",\n",
        "    cluster_range=range(2,8),\n",
        "):\n",
        "\n",
        "    # Create MLflow experiment\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    if distances is None:\n",
        "      # Initialize an empty array to store the distances\n",
        "      distances = np.zeros((time_series_data.shape[0], time_series_data.shape[0]))\n",
        "      # Calculate DTW distances between time series columns with progress tracking\n",
        "      for i in tqdm(range(time_series_data.shape[0]), desc=\"Calculating DTW Distances\"):\n",
        "        for j in range(i, time_series_data.shape[0]):\n",
        "            distance, _ = fastdtw(time_series_data[i], time_series_data[j])\n",
        "            distances[i, j] = distance\n",
        "            distances[j, i] = distance\n",
        "\n",
        "      print(\"DTW distances calculated.\")\n",
        "\n",
        "    # Silhouette Score vs. Cluster Count\n",
        "    silhouette_scores = []\n",
        "    inertia_scores = []\n",
        "\n",
        "    # Iterate through different cluster counts\n",
        "    for n_clusters in cluster_range:\n",
        "        # Create a unique experiment name for each cluster count\n",
        "        run_name = f\"{experiment_name}_{n_clusters}_Clusters\"\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name):\n",
        "            print(f\"Running experiment for {n_clusters} clusters...\")\n",
        "\n",
        "            # Perform K-Means clustering with DTW distances\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(distances)\n",
        "\n",
        "            # Get cluster labels\n",
        "            labels = kmeans.labels_\n",
        "\n",
        "            print(\"K-Means clustering completed.\")\n",
        "\n",
        "            # Log clustering results and distances to MLflow\n",
        "            mlflow.log_param(\"Cluster_Count\", n_clusters)\n",
        "            mlflow.log_param(\"Distance_Matrix_Shape\", distances.shape)\n",
        "            mlflow.log_param(\"Cluster_Labels\", labels.tolist())\n",
        "\n",
        "            print(\"Clustering results logged to MLflow.\")\n",
        "\n",
        "            # Calculate Silhouette Score and Inertia\n",
        "            silhouette = silhouette_score(distances, labels, metric=\"precomputed\")\n",
        "            inertia_score = kmeans.inertia_\n",
        "\n",
        "            silhouette_scores.append(silhouette)\n",
        "            inertia_scores.append(inertia_score)\n",
        "\n",
        "            mlflow.log_metric(\"Silhouette_Score\", silhouette)\n",
        "            mlflow.log_metric(\"Inertia_Score\", inertia_score)\n",
        "\n",
        "            print(f\"Silhouette Score: {silhouette}\")\n",
        "            print(f\"Inertia Score: {inertia_score}\")\n",
        "\n",
        "            # Define a color palette for plotting\n",
        "            palette = sns.color_palette(\"husl\", len(time_series_data))\n",
        "\n",
        "            # Visualize the clustered time series using Seaborn\n",
        "            num_rows = n_clusters\n",
        "            num_cols = 1\n",
        "            fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6 * num_rows))\n",
        "\n",
        "            for cluster_id in range(n_clusters):\n",
        "                ax = axes[cluster_id] if num_rows > 1 else axes\n",
        "                for series_idx, label in enumerate(labels):\n",
        "                    if label == cluster_id:\n",
        "                        series_name = merged_df.columns[series_idx + 1]  # Get the column name (series name)\n",
        "                        sns.lineplot(data=time_series_data[series_idx], color=palette[series_idx], label=series_name, ax=ax)\n",
        "\n",
        "                ax.annotate(f\"Inertia: {inertia_score:.2f}\", xy=(0.05, 0.85), xycoords='axes fraction', fontsize=10)\n",
        "                ax.annotate(f\"Silhouette: {silhouette:.2f}\", xy=(0.05, 0.75), xycoords='axes fraction', fontsize=10)\n",
        "\n",
        "                ax.set_title(f\"Cluster {cluster_id + 1}\")\n",
        "                ax.legend(loc='upper right')  # Add legends for series in the cluster\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot to a file and log it to MLflow\n",
        "            plot_path = f\"cluster_plots_{n_clusters}_clusters.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            mlflow.log_artifact(plot_path)\n",
        "\n",
        "            print(\"Cluster visualization plot saved and logged to MLflow.\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    with mlflow.start_run(run_name=\"Scores_logging\"):\n",
        "      # Log the lists of Silhouette and Inertia scores\n",
        "      mlflow.log_param(\"Silhouette_Scores\", silhouette_scores)\n",
        "      mlflow.log_param(\"Inertia_Scores\", inertia_scores)\n",
        "\n",
        "      # Silhouette Score vs. Cluster Count plot\n",
        "      plt.figure(figsize=(8, 4))\n",
        "      plt.plot(cluster_range, silhouette_scores, marker='o')\n",
        "      plt.title(\"Silhouette Score vs. Cluster Count\")\n",
        "      plt.xlabel(\"Cluster Count\")\n",
        "      plt.ylabel(\"Silhouette Score\")\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(\"silhouette_scores_plot.png\")\n",
        "      mlflow.log_artifact(\"silhouette_scores_plot.png\")\n",
        "\n",
        "      # Inertia Score vs. Cluster Count plot\n",
        "      plt.figure(figsize=(8, 4))\n",
        "      plt.plot(cluster_range, inertia_scores, marker='o')\n",
        "      plt.title(\"Inertia Score vs. Cluster Count\")\n",
        "      plt.xlabel(\"Cluster Count\")\n",
        "      plt.ylabel(\"Inertia Score\")\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(\"inertia_scores_plot.png\")\n",
        "      mlflow.log_artifact(\"inertia_scores_plot.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYJyc8BTvLNQ"
      },
      "source": [
        "#### Clustering Analysis (on Raw Series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzeaWQv6vLNR"
      },
      "outputs": [],
      "source": [
        "# Select the time series columns for clustering\n",
        "time_series_data = merged_df.iloc[:, 1:].values  # Exclude the 'date' column\n",
        "\n",
        "# Backfill missing values\n",
        "time_series_data = pd.DataFrame(time_series_data).fillna(method='bfill').values\n",
        "\n",
        "# Transpose the data to have time series in rows and measurements in columns\n",
        "time_series_data = time_series_data.T\n",
        "\n",
        "do_clustering(time_series_data, experiment_name=\"g_KMeans_Clustering\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCkX8FAvvLNR"
      },
      "source": [
        "#### Clustering Analysis (from 06/2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cknQs1swvLNR"
      },
      "outputs": [],
      "source": [
        "# Keep only data from 2021-06\n",
        "time_series_data = merged_df[merged_df['date'] >= '2021-06-01']\n",
        "\n",
        "# Select the time series columns for clustering\n",
        "time_series_data = merged_df.iloc[:, 1:].values  # Exclude the 'date' column\n",
        "\n",
        "# Backfill missing values\n",
        "time_series_data = pd.DataFrame(time_series_data).fillna(method='bfill').values\n",
        "\n",
        "# Transpose the data to have time series in rows and measurements in columns\n",
        "time_series_data = time_series_data.T\n",
        "\n",
        "do_clustering(time_series_data, experiment_name=\"g_KMeans_Clustering_from_2021-06\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOqmT3fuvLNR"
      },
      "source": [
        "#### Clustering Analysis (from 06/2021 and scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmgGfKxRvLNR"
      },
      "outputs": [],
      "source": [
        "# Keep only data from 2021-06\n",
        "time_series_data = merged_df[merged_df['date'] >= '2021-06-01']\n",
        "\n",
        "# Normalize the data\n",
        "def custom_min_max_scaling(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    scaled_column = (column - min_val) / (max_val - min_val)\n",
        "    return scaled_column\n",
        "\n",
        "time_series_data.iloc[:, 1:] = time_series_data.iloc[:, 1:].apply(custom_min_max_scaling, axis=0)\n",
        "\n",
        "# Select the time series columns for clustering\n",
        "time_series_data = time_series_data.iloc[:, 1:].values  # Exclude the 'date' column\n",
        "\n",
        "# Backfill missing values\n",
        "time_series_data = pd.DataFrame(time_series_data).fillna(method='bfill').values\n",
        "\n",
        "# Transpose the data to have time series in rows and measurements in columns\n",
        "time_series_data = time_series_data.T\n",
        "\n",
        "do_clustering(time_series_data, experiment_name=\"g_KMeans_Clustering_from_2021-06_normalized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he22pkvBvLNR"
      },
      "outputs": [],
      "source": [
        "# Create MLflow experiment\n",
        "mlflow.set_experiment(\"g_KMeans_Clustering_cropped_scaled_filtered\")\n",
        "\n",
        "# Keep only data from 2021-06\n",
        "time_series_data = merged_df[merged_df['date'] >= '2021-06-01']\n",
        "\n",
        "time_series_data.drop(['close_XLM/USD', 'close_RSR/USD', 'close_ICP/USD', 'close_LUNA/USD'], axis=1, inplace=True)\n",
        "\n",
        "# Normalize the data\n",
        "def custom_min_max_scaling(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    scaled_column = (column - min_val) / (max_val - min_val)\n",
        "    return scaled_column\n",
        "\n",
        "time_series_data.iloc[:, 1:] = time_series_data.iloc[:, 1:].apply(custom_min_max_scaling, axis=0)\n",
        "\n",
        "# Select the time series columns for clustering\n",
        "time_series_data = time_series_data.iloc[:, 1:].values  # Exclude the 'date' column\n",
        "\n",
        "# Backfill missing values\n",
        "time_series_data = pd.DataFrame(time_series_data).fillna(method='bfill').values\n",
        "\n",
        "# Transpose the data to have time series in rows and measurements in columns\n",
        "time_series_data = time_series_data.T\n",
        "\n",
        "do_clustering(time_series_data, experiment_name=\"g_KMeans_Clustering_from_2021-06_normalized_filtered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPZwSgVuwfID"
      },
      "source": [
        "#### Clustering Analysis (scaled and taken into account cryptos with 3 year data not NaN)\n",
        "\n",
        "05/2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiANqPzBwssz",
        "outputId": "3d960b14-8c5b-4466-f0cc-373f50c6bab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframe does not containt NaN values\n",
            "First date avaiable: 2022-03-08 18:00:00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def check_NaN_value(df):\n",
        "  nan_locations = df.isna()\n",
        "  nan_sum = nan_locations.sum().sum()\n",
        "  if nan_sum > 0:\n",
        "    print(f\"{nan_sum} NaN value still present\")\n",
        "  else:\n",
        "    print(\"Dataframe does not containt NaN values\")\n",
        "\n",
        "df = merged_df.copy()\n",
        "pruned_df = df.dropna()\n",
        "check_NaN_value(pruned_df)\n",
        "\n",
        "print(\"First date avaiable:\", pruned_df.iloc[0].date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR2cKbD1w08Y",
        "outputId": "56e7a9f9-3eaf-45a1-8636-3109dbfe1086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected 10 Cryptocurrencies with 3 Years of Data and No NaN Values:\n",
            "['close_ETH/USD', 'close_ADA/USD', 'close_LTC/USD', 'close_BTC/USD', 'close_XLM/USD', 'close_XRP/USD', 'close_BCH/USD', 'close_LINK/USD', 'close_DASH/USD', 'close_MATIC/USD']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to check if a cryptocurrency has continuous non-NaN data for 3 years\n",
        "def has_n_years_data(crypto_data, years):\n",
        "\n",
        "    crypto_data = crypto_data.dropna()\n",
        "    # Convert date date to Timestamps\n",
        "    timestamp_start = pd.to_datetime(crypto_data.iloc[0].date).value\n",
        "    timestamp_end = pd.to_datetime(crypto_data.iloc[-1].date).value\n",
        "\n",
        "    # Calculate the time difference between the first and last date in days\n",
        "    time_diff_in_days = (timestamp_end - timestamp_start) / (24 * 60 * 60 * 1e9)  # Convert nanoseconds to days\n",
        "\n",
        "    # Check if the time difference is greater than or equal to n years (approximately 3 * 365.25 days)\n",
        "    if time_diff_in_days < years * 365.25:\n",
        "        return False\n",
        "\n",
        "    # Check if there are any NaN values in the last n years of data (excluding the 'date' column)\n",
        "    last_years_data = crypto_data.iloc[-(int(years * 365.25)):, 1:]  # Extract the last 3 years of data\n",
        "    if last_years_data.isnull().values.any():\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Filter cryptocurrencies that meet the criteria\n",
        "valid_cryptos = []\n",
        "selected_columns = []\n",
        "years = 3\n",
        "\n",
        "for crypto_symbol in merged_df.columns[1:]:  # Exclude the 'date' column\n",
        "    crypto_data = merged_df[['date', crypto_symbol]]\n",
        "    if has_n_years_data(crypto_data, years):\n",
        "        valid_cryptos.append(crypto_symbol)\n",
        "\n",
        "# Select the top 10 cryptocurrencies based on the criteria (or fewer if there are fewer valid cryptocurrencies)\n",
        "selected_columns.append('date')\n",
        "selected_columns.extend(valid_cryptos)\n",
        "\n",
        "\n",
        "# Print the selected cryptocurrencies\n",
        "print(f\"Selected {len(valid_cryptos)} Cryptocurrencies with {years} Years of Data and No NaN Values:\")\n",
        "print(valid_cryptos)\n",
        "\n",
        "pruned_df = merged_df[selected_columns]\n",
        "pruned_df = pruned_df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxk5swTtw8Bf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create subplots with vertical space using Seaborn\n",
        "plt.figure(figsize=(20, 20))\n",
        "grid = plt.GridSpec(8, 3, hspace=1.5)  # Adjust the value of hspace as needed\n",
        "\n",
        "for i, col in enumerate(pruned_df.columns[1:]):\n",
        "    ax = plt.subplot(grid[i // 3, i % 3])\n",
        "    sns.lineplot(data=pruned_df, x=\"date\", y=col, ax=ax)\n",
        "    ax.set_title(col)\n",
        "\n",
        "    # Rotate x-axis labels to an oblique angle\n",
        "    plt.xticks(rotation=45)  # You can adjust the angle as needed\n",
        "\n",
        "    # Set x-axis limits to display the full date range\n",
        "    ax.set_xlim(pruned_df['date'].min(), pruned_df['date'].max())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU83cQNSC3yd"
      },
      "source": [
        "##### Min Max Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDmC9DnYw9JW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "time_series_data = pruned_df.copy()\n",
        "\n",
        "# Normalize the data\n",
        "def custom_min_max_scaling(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    scaled_column = (column - min_val) / (max_val - min_val)\n",
        "    return scaled_column\n",
        "\n",
        "time_series_data.iloc[:, 1:] = time_series_data.iloc[:, 1:].apply(custom_min_max_scaling, axis=0)\n",
        "\n",
        "# Select the time series columns for clustering\n",
        "time_series_data = time_series_data.iloc[:, 1:].values  # Exclude the 'date' column\n",
        "\n",
        "# Transpose the data to have time series in rows and measurements in columns\n",
        "time_series_data = time_series_data.T\n",
        "\n",
        "do_clustering(time_series_data, experiment_name=\"f_KMeans_Clustering_cropped_min_max_scaled_pruned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_OQufQjDF0K"
      },
      "source": [
        "##### Log Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbfIdERzDIRe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "time_series_data = pruned_df.copy()\n",
        "\n",
        "# Normalize the data\n",
        "def custom_log_scaling(column):\n",
        "    return np.log(column)\n",
        "\n",
        "time_series_data.iloc[:, 1:] = time_series_data.iloc[:, 1:].apply(custom_log_scaling, axis=0)\n",
        "\n",
        "# Select the time series columns for clustering\n",
        "time_series_data = time_series_data.iloc[:, 1:].values  # Exclude the 'date' column\n",
        "\n",
        "# Transpose the data to have time series in rows and measurements in columns\n",
        "time_series_data = time_series_data.T\n",
        "\n",
        "do_clustering(time_series_data, experiment_name=\"f_KMeans_Clustering_cropped_scaled_pruned_log_scaling\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4_ZXACDRvLNP",
        "bCkX8FAvvLNR"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
