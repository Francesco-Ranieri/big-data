{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../dev-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file if it exists\n",
    "# Don't use dotenv\n",
    "!pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD Experiment from MLFLOW\n",
    "\n",
    "Run \n",
    "\n",
    "``` python\n",
    "cd mlflow\n",
    "mlflow server\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "experiment_id = \"508906627986939289\"\n",
    "run_id = \"639b59a8bb2b476eb8a353a5ca4b6a66\"\n",
    "experiments = mlflow.search_runs(experiment_ids=experiment_id)\n",
    "\n",
    "# find in dataframe the experiment with run_id equals to 154a028019be42c9b78b3c05c8122e53\n",
    "experiment = experiments.loc[experiments['run_id'] == run_id]\n",
    "cluster_lables = json.loads(experiment[\"params.Cluster_Labels\"][1])\n",
    "criptos = json.loads(experiment[\"params.Criptocurrencies\"][1].replace(\"'\", '\"'))\n",
    "\n",
    "# Run without mlflow: uncomment the following lines and comment the previous ones\n",
    "# criptos = ['close_ADA/USD', 'close_BCH/USD', 'close_BTC/USD', 'close_DOGE/USD', 'close_DOT/USD', 'close_EOS/USD', 'close_ETC/USD', 'close_ETH/USD', 'close_LTC/USD', 'close_XRP/USD']\n",
    "# cluster_lables = [1, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "cripto_clusters = {}\n",
    "\n",
    "for label in cluster_lables:\n",
    "    cripto_clusters[label] = [criptos[i] for i, cluster_label in enumerate(cluster_lables) if cluster_label == label]\n",
    "\n",
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.loc[experiments['run_id'] == run_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = os.path.join(\"../airflow/assets\")\n",
    "dfs = []\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        dfs.append(pd.read_csv(os.path.join(folder, file), skiprows=1, parse_dates=['date']))\n",
    "print(dfs.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert \"date\" column to datetime in all dataframes\n",
    "for df in dfs:\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S', errors=\"coerce\")\n",
    "\n",
    "# Step 2: Find the oldest and newest dates across all dataframes\n",
    "all_dates = [df['date'] for df in dfs]\n",
    "all_dates_flat = [date for sublist in all_dates for date in sublist if not pd.isnull(date)]\n",
    "\n",
    "oldest_date = min(all_dates_flat)\n",
    "newest_date = max(all_dates_flat)\n",
    "\n",
    "# Step 3: Create a new dataframe with the date range\n",
    "date_range = pd.date_range(start=oldest_date, end=newest_date, freq='H')  # Hourly frequency\n",
    "merged_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Step 4: Add \"close\" columns from each dataframe to the merged_df using list comprehension\n",
    "for df in dfs:\n",
    "    try:\n",
    "        ticker = df['symbol'].iloc[0]  # Assuming each dataframe has a \"ticker\" column\n",
    "        close_col_name = f'close_{ticker}'\n",
    "\n",
    "        df = df.set_index('date').sort_index()\n",
    "        df = df[~df.index.duplicated(keep='first')].reindex(date_range, method='ffill')\n",
    "\n",
    "        # Create a DataFrame with the \"date\" and \"close\" columns\n",
    "        close_data = df[df.index.isin(date_range)][['close']]\n",
    "        close_data.rename(columns={'close': close_col_name}, inplace=True)\n",
    "\n",
    "        # Merge the \"close_data\" into the \"merged_df\"\n",
    "        merged_df = pd.merge(merged_df, close_data, left_on='date', right_index=True, how='left')\n",
    "    except ValueError as e:\n",
    "        print(f'Error on coin {ticker}: {e}')\n",
    "\n",
    "\n",
    "# Now, merged_df contains the desired data with the date range and \"close_{ticker}\" columns, with missing hours filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data in clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_data = {}\n",
    "merged_df = merged_df.dropna()\n",
    "# cripto_clusters\n",
    " \n",
    "# loop on key and value of cripto_clusters\n",
    "for cluster, criptos in cripto_clusters.items():\n",
    "    clusters_data[cluster] = merged_df[criptos]\n",
    "\n",
    "# Clusters now contains a dictionary with the cluster number as key and the dataframe with the criptos as value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price-oracle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
