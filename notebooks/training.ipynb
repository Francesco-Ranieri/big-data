{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET UP ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../dev-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file if it exists\n",
    "# Don't use dotenv\n",
    "!pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD Experiment from MLFLOW\n",
    "\n",
    "Run \n",
    "\n",
    "``` python\n",
    "cd mlflow\n",
    "mlflow server\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "\n",
    "experiment_id = \"508906627986939289\"\n",
    "run_id = \"639b59a8bb2b476eb8a353a5ca4b6a66\"\n",
    "\n",
    "# Mlfow Section\n",
    "experiments = mlflow.search_runs(experiment_ids=experiment_id)\n",
    "experiment = experiments.loc[experiments['run_id'] == run_id]\n",
    "cluster_lables = json.loads(experiment[\"params.Cluster_Labels\"][1])\n",
    "criptos = json.loads(experiment[\"params.Criptocurrencies\"][1].replace(\"'\", '\"'))\n",
    "\n",
    "# Run without mlflow: uncomment the following lines and comment the mlflow section\n",
    "# criptos = ['close_ADA/USD', 'close_BCH/USD', 'close_BTC/USD', 'close_DOGE/USD', 'close_DOT/USD', 'close_EOS/USD', 'close_ETC/USD', 'close_ETH/USD', 'close_LTC/USD', 'close_XRP/USD']\n",
    "# cluster_lables = [0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
    "\n",
    "cripto_clusters = {}\n",
    "\n",
    "for label in cluster_lables:\n",
    "    cripto_clusters[label] = [criptos[i] for i, cluster_label in enumerate(cluster_lables) if cluster_label == label]\n",
    "\n",
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = os.path.join(\"../airflow/assets\")\n",
    "dfs = []\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        dfs.append(pd.read_csv(os.path.join(folder, file), skiprows=1, parse_dates=['date']))\n",
    "print(dfs.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Convert \"date\" column to datetime in all dataframes\n",
    "for df in dfs:\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d %H:%M:%S', errors=\"coerce\")\n",
    "\n",
    "# Step 2: Find the oldest and newest dates across all dataframes\n",
    "all_dates = [df['date'] for df in dfs]\n",
    "all_dates_flat = [date for sublist in all_dates for date in sublist if not pd.isnull(date)]\n",
    "\n",
    "oldest_date = min(all_dates_flat)\n",
    "newest_date = max(all_dates_flat)\n",
    "\n",
    "# Step 3: Create a new dataframe with the date range\n",
    "date_range = pd.date_range(start=oldest_date, end=newest_date, freq='H')  # Hourly frequency\n",
    "merged_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Step 4: Add \"close\" columns from each dataframe to the merged_df using list comprehension\n",
    "for df in dfs:\n",
    "    try:\n",
    "        ticker = df['symbol'].iloc[0]  # Assuming each dataframe has a \"ticker\" column\n",
    "        close_col_name = f'close_{ticker}'\n",
    "\n",
    "        df = df.set_index('date').sort_index()\n",
    "        df = df[~df.index.duplicated(keep='first')].reindex(date_range, method='ffill')\n",
    "\n",
    "        # Create a DataFrame with the \"date\" and \"close\" columns\n",
    "        close_data = df[df.index.isin(date_range)][['close']]\n",
    "        close_data.rename(columns={'close': close_col_name}, inplace=True)\n",
    "\n",
    "        # Merge the \"close_data\" into the \"merged_df\"\n",
    "        merged_df = pd.merge(merged_df, close_data, left_on='date', right_index=True, how='left')\n",
    "    except ValueError as e:\n",
    "        print(f'Error on coin {ticker}: {e}')\n",
    "\n",
    "\n",
    "# Now, merged_df contains the desired data with the date range and \"close_{ticker}\" columns, with missing hours filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_data = {}\n",
    "merged_df = merged_df.dropna()\n",
    "# cripto_clusters\n",
    " \n",
    "# loop on key and value of cripto_clusters\n",
    "for cluster, criptos in cripto_clusters.items():\n",
    "    criptos.append('date')\n",
    "    clusters_data[cluster] = merged_df[criptos]\n",
    "\n",
    "# Clusters now contains a dictionary with the cluster number as key and the dataframe with the criptos as value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adfuller_test(time_series, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller (ADF) test for stationarity.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: A pandas Series or NumPy array containing the time series data.\n",
    "    - significance_level: The significance level for the test (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "    - ADF test result and p-value.\n",
    "    - A string indicating the stationarity based on the p-value.\n",
    "    \"\"\"\n",
    "\n",
    "    result = adfuller(time_series)\n",
    "    adf_statistic = result[0]\n",
    "    p_value = result[1]\n",
    "\n",
    "    if p_value <= significance_level:\n",
    "        stationarity = \"Stationary (p <= {0})\".format(significance_level)\n",
    "    else:\n",
    "        stationarity = \"Non-Stationary (p > {0})\".format(significance_level)\n",
    "\n",
    "    return adf_statistic, p_value, stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_and_plot_price_changes(data, column_name, window_size=10):\n",
    "    \"\"\"\n",
    "    Calculate and plot first-order percentage differences, rolling mean, and rolling standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A pandas DataFrame containing a 'date' index and the specified cryptocurrency price column.\n",
    "    - column_name: The name of the column representing cryptocurrency prices.\n",
    "    - window_size: The window size for the rolling statistics (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - None (plots the results).\n",
    "    \"\"\"\n",
    "    if 'date' not in data.columns:\n",
    "        raise ValueError(\"The 'data' DataFrame must contain a 'date' column as the index.\")\n",
    "\n",
    "    if column_name not in data.columns:\n",
    "        raise ValueError(f\"The specified column '{column_name}' is not found in the DataFrame.\")\n",
    "\n",
    "    # Calculate daily percentage changes\n",
    "    data['PriceChange'] = data[column_name].pct_change() * 100\n",
    "\n",
    "    # Calculate rolling mean and rolling standard deviation\n",
    "    data['RollingMean'] = data['PriceChange'].rolling(window=window_size).mean()\n",
    "    data['RollingStd'] = data['PriceChange'].rolling(window=window_size).std()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(data.index, data['PriceChange'], label='Price Change (%)', color='blue')\n",
    "    plt.plot(data.index, data['RollingMean'], label=f'Rolling Mean ({window_size}-hour)', color='green')\n",
    "    plt.plot(data.index, data['RollingStd'], label=f'Rolling Std Deviation ({window_size}-hour)', color='red')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Percentage Change / Rolling Statistics')\n",
    "    plt.title(f'{column_name} Price Changes and Rolling Statistics')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}\\n')\n",
    "    for cripto in cripto_clusters[cluster]:\n",
    "        if cripto != 'date':\n",
    "            print(f'Analyzing: {cripto}')\n",
    "\n",
    "            adf_statistic, p_value, stationarity = adfuller_test(clusters_data[cluster][cripto])\n",
    "            print(\"ADF Statistic:\", adf_statistic)\n",
    "            print(\"p-value:\", p_value)\n",
    "            print(\"Stationarity:\", stationarity)\n",
    "\n",
    "            calculate_and_plot_price_changes(clusters_data[cluster], cripto)\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Time Series - Vector Auto Regression (VAR)\n",
    "\n",
    "[Source](https://www.analyticsvidhya.com/blog/2018/09/multivariate-time-series-guide-forecasting-modeling-python-codes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "cluster = clusters_data[0]\n",
    "\n",
    "def mts_var_training(cluster_data):\n",
    "    \n",
    "    # Data preparation\n",
    "\n",
    "    # Step 1: Remove the \"date\" column\n",
    "    data = cluster_data.drop(['date'], axis=1)\n",
    "    cols = data.columns\n",
    "    # Step 2: Set the \"date\" column as the index\n",
    "    data.index = cluster_data.date\n",
    "\n",
    "    # Step 3: Dealing with missing values --> already done\n",
    "\n",
    "    # Step 4: Split the data into train and test sets\n",
    "    train_size = int(len(data) * 0.7)\n",
    "    test_size = (len(data) - train_size) / 2\n",
    "\n",
    "    train = data.iloc[0:train_size] \n",
    "    valid = data.iloc[train_size:int(len(data) - test_size)]\n",
    "    test = data.iloc[int(train_size + test_size):len(data)]\n",
    "\n",
    "    print('train size: ', len(train))\n",
    "    print('valid size: ', len(valid))\n",
    "    print('test size: ', len(test))\n",
    "    print('total size: ', len(data))\n",
    "    print(\"\\n\")\n",
    "    assert len(data) == len(train) + len(valid) + len(test)\n",
    "\n",
    "    model = VAR(endog=train)\n",
    "    model_fit = model.fit()\n",
    "    print(model_fit)\n",
    "\n",
    "    # make prediction on validation\n",
    "    prediction = model_fit.forecast(model_fit.endog, steps=len(valid))\n",
    "\n",
    "    #converting predictions to dataframe\n",
    "    pred = pd.DataFrame(index=range(0,len(prediction)),columns=[cols])\n",
    "    for j in range(0, len(cols)):\n",
    "        for i in range(0, len(prediction)):\n",
    "            pred.iloc[i][j] = prediction[i][j]\n",
    "\n",
    "    #check rmse\n",
    "    for i in cols:\n",
    "        print('RMSE value for', i, 'is : ', sqrt(mean_squared_error(pred[i], valid[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in cripto_clusters:\n",
    "    print(f'Cluster {cluster}: {cripto_clusters[cluster]}\\n')\n",
    "    mts_var_training(clusters_data[cluster])\n",
    "    print(\"\\n---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Time Series Forecasting with Deep Learning\n",
    "\n",
    "[Source 1](https://towardsdatascience.com/multivariate-time-series-forecasting-with-deep-learning-3e7b3e2d2bcf)\n",
    "[Source 2](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from math import sqrt\n",
    "\n",
    "# # Load your cryptocurrency price data (replace this with your own data)\n",
    "# data = clusters_data[0]\n",
    "# data['date'] = pd.to_datetime(data['date'])  # Convert the 'date' column to datetime\n",
    "\n",
    "# # Sort the data by date\n",
    "# data = data.sort_values(by='date')\n",
    "\n",
    "# # Define the number of previous time steps to consider for prediction\n",
    "# n_steps = 10  # You can adjust this value\n",
    "\n",
    "# # Create input data by shifting prices to create sequences\n",
    "# X = data.drop(columns=['date']).values\n",
    "# X_seq = [X[i:i + n_steps] for i in range(len(X) - n_steps)]\n",
    "\n",
    "# # Shift the closing price to predict the next closing price\n",
    "# y = data.drop(columns=['date']).shift(-n_steps).values\n",
    "\n",
    "# train_size = int(len(data) * 0.7)\n",
    "# test_size = (len(data) - train_size) / 2\n",
    "\n",
    "# train = data.iloc[0:train_size] \n",
    "# valid = data.iloc[train_size:int(len(data) - test_size)]\n",
    "# test = data.iloc[int(train_size + test_size):len(data)]\n",
    "\n",
    "# print('train size: ', len(train))\n",
    "# print('valid size: ', len(valid))\n",
    "# print('test size: ', len(test))\n",
    "# print('total size: ', len(data))\n",
    "# print(\"\\n\")\n",
    "# assert len(data) == len(train) + len(valid) + len(test)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_seq, y[:-n_steps], test_size=0.2, shuffle=False)\n",
    "\n",
    "# # split the training set into training and validation sets\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# # Reshape the data to 3D for LSTM\n",
    "# X_train = np.array(X_train)\n",
    "# X_test = np.array(X_test)\n",
    "# y_train = np.array(y_train)\n",
    "# y_test = np.array(y_test)\n",
    "\n",
    "# # Create a function to calculate RMSE\n",
    "# def calculate_rmse(y_true, y_pred):\n",
    "#     return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# # Define the model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.LSTM(64, activation='relu', input_shape=(n_steps, X_train.shape[2]), return_sequences=True),\n",
    "#     tf.keras.layers.LSTM(64, activation='relu'),\n",
    "#     tf.keras.layers.Dense(X_train.shape[2])\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate and print RMSE\n",
    "# rmse = calculate_rmse(y_test, y_pred)\n",
    "# print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price-oracle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
